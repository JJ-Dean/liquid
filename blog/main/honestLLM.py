# -*- coding: utf-8 -*-
"""HonestLLM_1.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u23t39WlvTNiwi1EiHiQORyosrMo1AIR
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install tiktoken
!pip install openai
!pip install newspaper3k

# imports
import openai  # for calling the OpenAI API
import pandas as pd  # for storing text and embeddings data
import tiktoken  # for counting tokens
import re
from tqdm import tqdm
import json
from scipy import spatial  # for calculating vector similarities for search

from openai import OpenAI
import os
import sys
import urllib.request

from newspaper import Article

import os
os.environ["OPENAI_API_KEY"] = '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'

import openai
openai.api_key = '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'

"""#  유저 질문에서 키워드 추출하기"""

def find_keyword(openai : any, question : str) -> str:
    response = client.chat.completions.create(
      model="gpt-4-1106-preview",
      messages=[
          #시스템 메시지는 어시스턴트의 동작을 설정(선택사항)
        {"role": "system", "content": "A system that extracts only one keyword from a given question."},
          #어시스턴트가 응답할 수 있는 요청이나 의견을 제공
        {"role": "user", "content": f"""
        #######prompt########
        Generate only one keyword from the given question.
        question : {question}

        ####example####
        question : '손흥민의 최근 뉴스들을 알려줘'
        answer : '손흥민'

        question : '미국의 경제 상황에 대해서 알려줘'
        answer : '미국경제'

        ####output####
        answer :

         """},
      ]
    )
    return response.choices[0].message.content

#find_keyword(openai,'한국의 경제상황에 대해 알려줘')

"""# 기사 불러오기"""

import os
import sys
import urllib.request


def search_article(keyword:str):
  client_id = "mlh83KWXXlaNBjyKSkwl"
  client_secret = "FXklZGr5en"
  encText = urllib.parse.quote(keyword)

  #url = "https://openapi.naver.com/v1/search/blog?query=" + encText # JSON 결과
  # url = "https://openapi.naver.com/v1/search/blog.xml?query=" + encText # XML 결과
  url = "https://openapi.naver.com/v1/search/news.json?query=" + encText

  request = urllib.request.Request(url)
  request.add_header("X-Naver-Client-Id",client_id)
  request.add_header("X-Naver-Client-Secret",client_secret)
  response = urllib.request.urlopen(request)
  rescode = response.getcode()

  if(rescode==200):
    response_body = response.read()
    #print(response_body.decode('utf-8'))
  else:
      print("\n기사를 찾을 수 없습니다. can't find the article. \nError Code:\n" + rescode)
  return response_body

"""# 기사 임베딩하기"""

def get_embedding(text, model="text-embedding-ada-002"):
    text = text.replace("\n", " ")
    return client.embeddings.create(input = [text], model=model).data[0].embedding

def make_article(response_body):
  ## 기사 api 받은 데이터 ##
  text = response_body.decode('utf-8')

  # 정규 표현식 패턴
  pattern = r'"link":"(https[^"]+)"'

  # 정규 표현식을 사용하여 기사 링크만 추출
  links = re.findall(pattern, text)

  #'\'문자 제거
  links = [text.replace("\\","") for text in links]


  ###데이터프레임으로 만들기###
  articles = pd.DataFrame(columns=['title','text','title_embedding', 'text_embedding'])

  for i, link in enumerate(links) :

    #news_url = "https://www.wikitree.co.kr/articles/900115"
    news_url = link

    #article 클래스를 설정하고, 클래스 안에 url을 넣어준다.
    article = Article(news_url, language='ko')

    #다운로드와 파싱 제공
    article.download()
    article.parse()

    #제목 모음
    articles.loc[i] = [article.title, article.text,[],[]]


  ###임베딩까지###
  client = OpenAI()

  EMBEDDING_MODEL = "text-embedding-ada-002"
  title_data = articles['title'].to_list()
  text_data = articles['text'].to_list()
  for i in range(len(title_data)):
    title_response = get_embedding(title_data[i])
    articles['title_embedding'][i] = title_response

    text_response = get_embedding(text_data[i])
    articles['text_embedding'][i] = text_response

  return articles

"""# 유저 질문과의 유사도 계산하기"""

# search function
def strings_ranked_by_relatedness(
    query: str, #쿼리문 입력
    df: pd.DataFrame, #데이터프레임
    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y), #코사인 유사도 계산
    top_n: int = 100
) -> tuple[list[str], list[float]]:
    """Returns a list of strings and relatednesses, sorted from most related to least.
    """
    #가장 관련성이 높은 것부터 가장 낮은 것까지 정렬된 문자열 및 관련성 목록을 반환합니다.
    #query_embedding_response = get_embedding([query], model='text-embedding-ada-002')

    query_embedding = get_embedding(query) #쿼리문 임베딩
    strings_and_relatednesses = [ #text와 유사도 계산
        (row["title"], row["text"], relatedness_fn(query_embedding, row["title_embedding"]))
        for i, row in df.iterrows()
    ]
    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)
    title_strings, text_strings, relatednesses = zip(*strings_and_relatednesses)
    return title_strings[:top_n], text_strings[:top_n], relatednesses[:top_n]

def collect_articles(keyword : str, df : pd.DataFrame, top_n: int = 100):
  total_articles = []

  title_strings, text_strings, relatednesses = strings_ranked_by_relatedness("손흥민", articles, top_n=5)
  for title_strings, text_strings,  relatedness in zip(title_strings, text_strings,relatednesses):
    #print(f"{relatedness=:.3f}")
    #display(title_strings)
    #display(text_strings)
    total_articles.append(text_strings)

  return total_articles

#collect_articles("손흥민", articles, top_n=5)

"""## OpenAI API활용해 기사를 기반으로 질문에 답변하기"""

from openai import OpenAI
client = OpenAI()

def generate_data(openai : any, article: str, question : str) -> str:
    response = client.chat.completions.create(
      model="gpt-4-1106-preview",
      messages=[
          #시스템 메시지는 어시스턴트의 동작을 설정(선택사항)
        {"role": "system", "content": "It's a system that generates relevant answers based on news stories related to your question."},
          #어시스턴트가 응답할 수 있는 요청이나 의견을 제공
        {"role": "user", "content": f"""
        #######prompt########
        Read an {article} related to a given {question} and answer it appropriately.
         """},

      ]
    )
    
    return response.choices[0].message.content

def generate_answer(question : str):
  #질문에 대한 키워드 찾기
  keyword = find_keyword(openai, question)

  #기사 검색하기 (네이버 api)
  response = search_article(keyword)

  #검색된 기사 임베딩
  article = make_article(response)

  #질문과 유사한 기사 찾기
  most_related_article = collect_articles(keyword, article, top_n=5)

  #질문에 답변하기
  response = generate_data(openai,most_related_article, question)

  return response

answer = generate_answer('손흥민의 최근 뉴스들을 알려줘')

answer
